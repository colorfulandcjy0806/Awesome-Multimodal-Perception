# Awesome-Multimodal-Perception ğŸŒˆ

å¦‚æœä½ å¸Œæœ›é˜…è¯»æœ¬æ–‡æ¡£çš„ä¸­æ–‡ç‰ˆæœ¬ï¼Œè¯·ç‚¹å‡»[è¿™é‡Œ](./Readme.md)ã€‚ 

If you would like to read the English version of this document, please click [Here](./English.md).

Welcome to the `Awesome-Multimodal-Perception` repository, a collection of influential and inspiring papers I've encountered in my study and research in the field of multimodal perception. Multimodal perception technology is key to achieving the perceptual capabilities of artificial intelligence systems, covering a wide range of technologies from image and video analysis to speech and text understanding. Research in this field helps to advance developments in machine learning, human-computer interaction, natural language processing, and more.

In this repository, you will find a list of papers that I consider to be the most impactful and enlightening, categorized by type, along with links to the papers and their code, as well as some insightful interpretations, hoping to aid in your learning and research endeavors.

## ğŸ“– Papers List

| No.  | Type            | Paper Title                                                  | Authors                                                      | Publishing Institution           | Journal/Conference  | Paper Link                            | Code Link                                          | My Interpretation                           |
| ---- | --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------- | ------------------- | ------------------------------------- | -------------------------------------------------- | ------------------------------------------- |
| 1    | Paintingèåˆ    | VirtualPainting: Addressing Sparsity with Virtual Points and Distance-Aware Data Augmentation for 3D Object Detection | Sudip Dhakal,Dominic Carrillo,Deyuan Qu,Michael Nutt,Qing Yang,Song Fu | University of North Texas Denton | æ—                   | [ğŸ“„](https://arxiv.org/abs/2312.16141) | [ğŸ’»](https://arxiv.org/abs/2312.16141)              | [ğŸ”](https://zhuanlan.zhihu.com/p/685337158) |
| 2    | Transformeræ¶æ„ | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | Ze Liu,Yutong Lin,Yue Cao,Han Hu,Yixuan Wei,Zheng Zhang,Stephen Lin,Baining Guo | Microsoft Research Asia          | ICCV2021 Best paper | [ğŸ“„](https://arxiv.org/abs/2103.14030) | [ğŸ’»](https://github.com/microsoft/Swin-Transformer) | [ğŸ”](https://zhuanlan.zhihu.com/p/685551585) |
| 3 | 3Dç›®æ ‡æ£€æµ‹ | UniMODE: Unified Monocular 3D Object Detection | Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao | The University of Hong Kongã€Zhejiang Universityã€University of Central Florida | CVPR2024 | [ğŸ“„](https://arxiv.org/abs/2402.18573) | [ğŸ’»](https://arxiv.org/abs/2402.18573) | [ğŸ”](https://zhuanlan.zhihu.com/p/686228362) |
| 4 | ç›¸æœº+æ¿€å…‰é›·è¾¾èåˆ | DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection | Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Bo Wu, Yifeng Lu, Denny Zhou, Quoc V. Le, Alan Yuille, Mingxing Tan | Johns Hopkins Universityã€Google | CVPR2022 | [ğŸ“„](https://arxiv.org/abs/2203.08195v1) | [ğŸ’»](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/car/deep_fusion.py) | [ğŸ”](https://zhuanlan.zhihu.com/p/687676198) |
| 5 | 3Dç›®æ ‡æ£€(çº¯è§†è§‰) | Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors | Haoxuanye Ji,Pengpeng Liang,Erkang Cheng | éƒ‘å·å¤§å­¦ã€Nullmax | CVPR2024 | [ğŸ“„](https://arxiv.org/abs/2403.06093) | [ğŸ’»](https://arxiv.org/abs/2403.06093) | [ğŸ”](https://zhuanlan.zhihu.com/p/688934983) |
| 6 | 3Dç›®æ ‡æ£€(çº¯è§†è§‰) | Object as Query: Lifting any 2D Object Detector to 3D Detection | Zitian Wang,Zehao Huang,Jiahui Fu,Naiyan Wang,Si Liu | åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦äººå·¥æ™ºèƒ½ç ”ç©¶é™¢ã€å›¾æ£®æœªæ¥ | ICCV2023 | [ğŸ“„](https://arxiv.org/abs/2301.02364) | [ğŸ’»](https://github.com/tusen-ai/MV2D?tab=readme-ov-file) | [ğŸ”](https://zhuanlan.zhihu.com/p/690036659) |
| 7 | 3Dç›®æ ‡æ£€(çº¯è§†è§‰) | MonoCD: Monocular 3D Object Detection with Complementary Depths | Longfei Yan, Pei Yan, Shengzhou Xiong, Xuanyu Xiang, Yihua Tan | åä¸­ç§‘æŠ€å¤§å­¦äººå·¥æ™ºèƒ½ä¸è‡ªåŠ¨åŒ–å­¦é™¢ | CVPR2024 | [ğŸ“„](https://arxiv.org/abs/2404.03181) | [ğŸ’»](https://github.com/elvintanhust/MonoCD) | [ğŸ”](https://zhuanlan.zhihu.com/p/691322485) |
| ...  | ...             | ...                                                          | ...                                                          | ...                              | ...                 | ...                                   | ...                                                | ...                                         |

## ğŸ¤ How to Contribute

This project welcomes any form of contribution, whether it's adding new papers, providing interpretations, adding code links, or improving the structure of the repository. If you have any ideas or resources you'd like to share, please submit them through issues or pull requests.

## ğŸŒŸ Acknowledgements

Thank you to all the researchers and developers who have contributed to the field of multimodal perception. Your work has greatly advanced this field and provided us with a wealth of learning resources.

---

I hope this repository becomes a valuable resource on your journey of learning and researching multimodal perception. If you find the content here helpful, don't hesitate to star the repo! ğŸŒŸ
