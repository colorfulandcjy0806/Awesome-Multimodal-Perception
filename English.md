# Awesome-Multimodal-Perception 🌈

如果你希望阅读本文档的中文版本，请点击[这里](./Readme.md)。 

If you would like to read the English version of this document, please click [Here](./English.md).

Welcome to the `Awesome-Multimodal-Perception` repository, a collection of influential and inspiring papers I've encountered in my study and research in the field of multimodal perception. Multimodal perception technology is key to achieving the perceptual capabilities of artificial intelligence systems, covering a wide range of technologies from image and video analysis to speech and text understanding. Research in this field helps to advance developments in machine learning, human-computer interaction, natural language processing, and more.

In this repository, you will find a list of papers that I consider to be the most impactful and enlightening, categorized by type, along with links to the papers and their code, as well as some insightful interpretations, hoping to aid in your learning and research endeavors.

## 📖 Papers List

| No.  | Type            | Paper Title                                                  | Authors                                                      | Publishing Institution           | Journal/Conference  | Paper Link                            | Code Link                                          | My Interpretation                           |
| ---- | --------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------------------------------- | ------------------- | ------------------------------------- | -------------------------------------------------- | ------------------------------------------- |
| 1    | Painting融合    | VirtualPainting: Addressing Sparsity with Virtual Points and Distance-Aware Data Augmentation for 3D Object Detection | Sudip Dhakal,Dominic Carrillo,Deyuan Qu,Michael Nutt,Qing Yang,Song Fu | University of North Texas Denton | 无                  | [📄](https://arxiv.org/abs/2312.16141) | [💻](https://arxiv.org/abs/2312.16141)              | [🔍](https://zhuanlan.zhihu.com/p/685337158) |
| 2    | Transformer架构 | Swin Transformer: Hierarchical Vision Transformer using Shifted Windows | Ze Liu,Yutong Lin,Yue Cao,Han Hu,Yixuan Wei,Zheng Zhang,Stephen Lin,Baining Guo | Microsoft Research Asia          | ICCV2021 Best paper | [📄](https://arxiv.org/abs/2103.14030) | [💻](https://github.com/microsoft/Swin-Transformer) | [🔍](https://zhuanlan.zhihu.com/p/685551585) |
| 3 | 3D目标检测 | UniMODE: Unified Monocular 3D Object Detection | Zhuoling Li, Xiaogang Xu, SerNam Lim, Hengshuang Zhao | The University of Hong Kong、Zhejiang University、University of Central Florida | CVPR2024 | [📄](https://arxiv.org/abs/2402.18573) | [💻](https://arxiv.org/abs/2402.18573) | [🔍](https://zhuanlan.zhihu.com/p/686228362) |
| 4 | 相机+激光雷达融合 | DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection | Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Bo Wu, Yifeng Lu, Denny Zhou, Quoc V. Le, Alan Yuille, Mingxing Tan | Johns Hopkins University、Google | CVPR2022 | [📄](https://arxiv.org/abs/2203.08195v1) | [💻](https://github.com/tensorflow/lingvo/blob/master/lingvo/tasks/car/deep_fusion.py) | [🔍](https://zhuanlan.zhihu.com/p/687676198) |
| ...  | ...             | ...                                                          | ...                                                          | ...                              | ...                 | ...                                   | ...                                                | ...                                         |

## 🤝 How to Contribute

This project welcomes any form of contribution, whether it's adding new papers, providing interpretations, adding code links, or improving the structure of the repository. If you have any ideas or resources you'd like to share, please submit them through issues or pull requests.

## 🌟 Acknowledgements

Thank you to all the researchers and developers who have contributed to the field of multimodal perception. Your work has greatly advanced this field and provided us with a wealth of learning resources.

---

I hope this repository becomes a valuable resource on your journey of learning and researching multimodal perception. If you find the content here helpful, don't hesitate to star the repo! 🌟
